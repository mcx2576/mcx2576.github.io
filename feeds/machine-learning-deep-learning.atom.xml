<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Cecilia's Wonderland - Machine Learning, Deep Learning</title><link href="/" rel="alternate"></link><link href="/feeds/machine-learning-deep-learning.atom.xml" rel="self"></link><id>/</id><updated>2020-05-24T00:00:00+02:00</updated><entry><title>How the curse of dimensionality kills classic statistical methods?</title><link href="/how-the-curse-of-dimensionality-kills-classic-statistical-methods.html" rel="alternate"></link><published>2020-05-24T00:00:00+02:00</published><updated>2020-05-24T00:00:00+02:00</updated><author><name>Cecilia Miao</name></author><id>tag:None,2020-05-24:/how-the-curse-of-dimensionality-kills-classic-statistical-methods.html</id><summary type="html">&lt;p&gt;What is the curse of dimensionality and how does it happen in the classic machine learning models?&lt;/p&gt;</summary><content type="html">&lt;p&gt;translation Is: false
status: draft&lt;/p&gt;
&lt;h1&gt;The curse of dimensionality&lt;/h1&gt;
&lt;p&gt;The curse of dimensionality indicates the phenomenon that as the input parameters of a model increase, the volume of the spaces increases exponentially that the existing data becomes sparse. Inevitably, &lt;strong&gt;Statistical Inference&lt;/strong&gt; methods lose their power under this situation.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Example:&lt;/em&gt; Assuming you that are a kid, we always play the game "rock, paper, scissors".  If we want to predict the probability of wining the game based on past gaming experiences with our friend Alex, then we need to gather some data to predict the probability of him to take a certain move. Then, when some other friends Carlotta, John and Linda join the game, the prediction becomes harder and harder, as we need to not only predict the conditional probability of our serves, but their conditional probabilities and joint probabilities. In the end, we need to play far more rounds in order to win this game.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;The 'cure' for this issue&lt;/h1&gt;
&lt;h2&gt;PCA&lt;/h2&gt;
&lt;p&gt;Some methods such as &lt;strong&gt;PCA&lt;/strong&gt;(principle component analysis)  could help the feature engineering process by decreasing the number of attributes without excluding significant factors within the model. &lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Example:&lt;/em&gt; If you are a fan of music, there must be a type of music that makes you happy the most. The style of the music could be decided by different features, the chords, the rhythm, the melody and the strength of the music, etc. These features  might &lt;strong&gt;jointly&lt;/strong&gt; decide the style of the music, which means that you can't easily decide the style just by one or two individual features. However, some features might be redundant to decide a style of music, like the specific node in a music piece, this information is already cover in chords, or correlated with other keys. How do we smartly decide important features to use for avoiding the &lt;strong&gt;curse of dimensionality&lt;/strong&gt; and effectively deciding a specific style of music.
The PCA could be calculated with the following procedure.
1. Calculate the covariance matrix of all features
2. Find the eigen vector of the matrix 
3. Rank eigen vectors based on eigen values
4. Choose the most important eigen vectors as our feature vectors.
So, what does each of the step mean to a layman?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Calculate the covariance matrix of all features&lt;/h3&gt;
&lt;p&gt;The covariance matrix could explicitly describe how much each feature correlated with other features. For instance, we would imagine that the rhythm of a music piece is highly correlated with the beat, while it might be less correlated to the melody. We try to contract table/matrix to calculate the covariance of all features in order to obtain the eigen vector.&lt;/p&gt;
&lt;h3&gt;Find the eigen vectors of the matrix&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Eigen vector&lt;/strong&gt; could be regarded as the coordinate mapping of all feature points in the feature space, so that the sum of square distance of original points are minimized after the mapping. How do we preserve the difference between different features at the same time? Eigen vector also has the property of maximizing the variance of a dataset. So that we know some major features such as rhythm, melody and strength will be represented by those eigen vectors. The &lt;strong&gt;minimum square error&lt;/strong&gt; and the &lt;strong&gt;maximum variance&lt;/strong&gt; will be achieved at the same time in PCA.&lt;/p&gt;
&lt;h3&gt;Rank eigen vectors based on eigen values&lt;/h3&gt;
&lt;h3&gt;Choose the most important eigen vectors as our feature vectors.&lt;/h3&gt;
&lt;p&gt;After you choose the eigen vectors, you can use them for further predictions or analysis.&lt;/p&gt;</content><category term="Machine Learning, Deep Learning"></category><category term="Machine Learning"></category><category term="Statistical Inferences"></category></entry></feed>