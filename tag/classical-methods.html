<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>Cecilia's Wonderland - Classical methods</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Cecilia's Wonderland Atom Feed" />
</head>

<body id="index" class="home">
<a href="http://github.com/mcx2576s">
<img style="position: absolute; top: 0; right: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_right_red_aa0000.png" alt="Fork me on GitHub" />
</a>
        <header id="banner" class="body">
                <h1><a href="/">Cecilia's Wonderland</a></h1>
                <nav><ul>
                    <li><a href="/pages/about.html">About</a></li>
                    <li><a href="/category/misc.html">misc</a></li>
                    <li><a href="/category/supervised-methods.html">Supervised methods</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/decision-trees-can-we-make-it-easier.html">Decision trees, can we make it easier?</a></h1>
<footer class="post-info">
        <abbr class="published" title="2020-05-23T00:00:00+02:00">
                Published: Sat 23 May 2020
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/cecilia.html">Cecilia</a>
        </address>
<p>In <a href="/category/supervised-methods.html">Supervised methods</a>.</p>
<p>tags: <a href="/tag/machine-learning.html">Machine Learning</a> <a href="/tag/classical-methods.html">Classical methods</a> <a href="/tag/supervised-learning.html">Supervised Learning</a> </p>
</footer><!-- /.post-info --><p>translation Is: false
status: draft</p>
<h1>What are decision trees?</h1>
<p>Decision trees are tree structured algorithms which map the input feature variables to a specific target. The algorithm is implemented with tree-structure, where at each node is a decision point and the end-nodes will be equipped with right and left leaves. The target variable could be discrete or continuous, which results in the classification trees and regression trees.</p>
<h1>Explain decision trees to a child</h1>
<h2>The decision part</h2>
<p>imagining that you are playing the monopoly game. You need to make a lot of decisions during this process, for instance, at the beginning of the game, you need to decide whether you should spend the money to buy more assets or spend it to upgrade your house. Based on each of the decision that you made, you could encounter more subsequent decisions. In the end, you would be directed to the result of winning or losing the game.
Therefore, in order to win this game, you should estimate the probability of each decision leading to the final results or wining and losing the game. After you played a lot of games, you could gradually learn to make the best decision that could lead to your triumph, this process is called parameter updates.</p>
<h2>The tree part</h2>
<p>Probably you could be tired of playing so many games. it will be great if there is a robot that could help you to automatically play the game. How can we tell the computer to calculate all results and make the right decision for us? if we draw this process on a blank paper, you could probability observe this kind of representation.</p>
<p>Look at the picture, if you rotate it a little bit, doesn't it just look like a normal tree. Basically, it has the root since the beginning of the game, branch for each decision point, and leaf for the ending results. We could represent it with a specific computer data structure-tree in order to solve it. The computer could use that data structure to compute all results and update the probability of choosing the left or right branch for you, Then you could easily win this game.</p>
<h1>Technical details</h1>
<p>Above-mentioned explanations could already be sufficient to explain to a child about what decision trees look like. However, since we are enthusiastic machine learning practitioners, we would like to more technical details. For instance, based on what criteria does the tree split, what's the difference between a classification tree and regression trees, are there any variations in different decision trees?</p>
<h2>Decision tree algorithms</h2>
<ol>
<li>ID3 (Iterative Dichotomiser 3)</li>
<li>CART (Classification And Regression Tree)</li>
<li>Chi-square automatic interaction detection (CHAID)</li>
</ol>
<p>These algorithms differentiate with each other mainly based on different impurity measures. 
<strong>Impurity:</strong> impurity measures the homogeneity of the labels of that node. if a node has only one label, we would say its impurity is 0.
<strong>Information gain</strong> Information gain is the difference between the parent node impurity and the weighted sum of the two child node impurities. The
<strong>ID3:</strong> ID3 mainly uses <strong>entropy</strong> to measure the information gain by each split. The entropy could be calculated by the entropy gain $-t_ilog_2(t_i)$ across all classes (t_i includes the class t_1, t_2, t_3...).
<strong>CART:</strong> Classification and regression trees are most classic decision tree algorithms. their calculation are also pretty intuitive. For <strong>Classification</strong>, the model uses <strong>Gini index</strong> to measure the impurity of each node. <strong>Gini index</strong> is calculated by aggregate the frequency calculation of each class, $-f_i(1-f_i)$. Where f_I is the frequency of i class appearing in the specific node. The higher the  <strong>Gini index</strong>, the higher the homogeneity  for the specific tree.
<strong>Regression</strong>, on the other hand, uses a totally different impurity measures to calculate the impurity-variance. In general, variance is calculated by $1/NSUM^N_{I=1}(y_i-\mu)^2$, where $y_I$ is the label of that instance while $\mu$ is the mean of labels (1/NSUM^N_{I=1}y_i).
__CHAID:__CHAID uses chi-square to measure impurity. The higher the chi-square value, the higher the statistical significance between sub-node and parent node. Chi-square value in statistics is always calculated by $\sqrt{(Actual_observation-Expected_observation)^2/expected_observation}$. It could be used for both classification and regression trees.</p>                </article>
            </aside><!-- /#featured -->
        <section id="extras" class="body">
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                            <li><a href="http://github.com/mcx2576">github</a></li>
                            <li><a href="https://www.facebook.com/mcx2576">facebook</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>